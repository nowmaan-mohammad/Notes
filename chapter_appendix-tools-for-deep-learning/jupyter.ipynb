{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2vLifH1snvz",
        "outputId": "a6f26eed-ee4a-48b7-8b85-c28e2f93f91f"
      },
      "id": "j2vLifH1snvz",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTd1OCIgspr6",
        "outputId": "e0572365-3a64-4c2a-83bd-358f7635f7ec"
      },
      "id": "UTd1OCIgspr6",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.28\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "-SjYH53isuhf"
      },
      "id": "-SjYH53isuhf",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_session=SparkSession.builder.appName('test').getOrCreate()"
      ],
      "metadata": {
        "id": "ydCJHKy3s5K4"
      },
      "id": "ydCJHKy3s5K4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_session.conf.get('spark.app.id'))\n",
        "print(spark_session.conf.get('spark.app.name'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g1mvBdhuGeI",
        "outputId": "1b143cb2-c34e-4b8e-e4bc-5c9704a88e1a"
      },
      "id": "4g1mvBdhuGeI",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local-1755428994284\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spark_session.sparkContext.getConf().getAll()"
      ],
      "metadata": {
        "id": "qWGHtNIZukPK"
      },
      "id": "qWGHtNIZukPK",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, regexp_extract\n",
        "\n",
        "data = [(1, 'Winston', 'winston@leetcode.COM')]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"mail\", StringType(), True)\n",
        "])\n",
        "\n",
        "users_df = spark_session.createDataFrame(data, schema)\n",
        "\n",
        "# Filter for emails containing '@leetcode.com' and matching the specified prefix pattern\n",
        "users_df = users_df.filter(col(\"mail\").rlike(\"^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode\\\\.com$\"))\n",
        "\n",
        "users_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ydjmrIBhWY",
        "outputId": "dd9dd7e2-8f46-4ff9-bdd6-16ca2703c115"
      },
      "id": "S7ydjmrIBhWY",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------------------+\n",
            "|user_id|name     |mail                   |\n",
            "+-------+---------+-----------------------+\n",
            "|1      |Winston  |winston@leetcode.com   |\n",
            "|3      |Annabelle|bella-@leetcode.com    |\n",
            "|4      |Sally    |sally.come@leetcode.com|\n",
            "+-------+---------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "data = [(1, 'Abbot'),\n",
        "        (2, 'Doris'),\n",
        "        (3, 'Emerson'),\n",
        "        (4, 'Green'),\n",
        "        (5, 'Jeames')]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"student\", StringType(), True)\n",
        "])\n",
        "\n",
        "students_df = spark_session.createDataFrame(data, schema)\n",
        "students_df=students_df.s"
      ],
      "metadata": {
        "id": "CwznOu8IIQs9"
      },
      "id": "CwznOu8IIQs9",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, max\n",
        "\n",
        "# Get the maximum id from the students_df\n",
        "max_id_df = students_df.agg(max(\"id\").alias(\"max_id\"))\n",
        "max_id = max_id_df.collect()[0][\"max_id\"]\n",
        "\n",
        "# Apply the logic to swap ids\n",
        "reordered_students_df = students_df.withColumn(\n",
        "    \"id\",\n",
        "    when((col(\"id\") % 2 == 1) & (col(\"id\") + 1 <= max_id), col(\"id\") + 1)\n",
        "    .when(col(\"id\") % 2 == 0, col(\"id\") - 1)\n",
        "    .otherwise(col(\"id\"))\n",
        ")\n",
        "\n",
        "# Order the result by the new id\n",
        "reordered_students_df = reordered_students_df.orderBy(\"id\")\n",
        "\n",
        "reordered_students_df.show()"
      ],
      "metadata": {
        "id": "eZUiOWROT5ts"
      },
      "id": "eZUiOWROT5ts",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}